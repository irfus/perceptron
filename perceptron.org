#+TEMPLATE: ACL
#+key: acl
#+group: manuscript
#+contributor: Irfan S <irfans2@illinois.edu>
#+default-filename: draft.org

#+TITLE: Single Cell Perceptron in Python: Implementation and Results

#+latex_class: article-no-defaults
#+OPTIONS: |:nil toc:nil author:nil
#+latex_class_options: [11pt,a4paper]
#+latex_header: \usepackage[utf8]{inputenc}
#+latex_header: \usepackage[hyperref]{acl2017}
#+latex_header: \usepackage{times}
#+latex_header: \usepackage{minted}
#+latex_header: \usepackage{float}
#+latex_header: \usepackage{latexsym}
#+latex_header: \usepackage{graphicx}
#+latex_header: \usepackage{url}
#+latex_header: \usepackage{cleveref}
#+latex_header: \aclfinalcopy
#+latex_header: \author{Irfan S \\ Dept. of Linguitics / UIUC \\ {\tt irfans2@illinois.edu}}

#+latex_header: \newcommand\BibTeX{B{\sc ib}\TeX}
#+EXPORT_EXCLUDE_TAGS: noexport
#+DRAWERS: NOTES

\maketitle

* Data
We will be using five datasets to train and observe the model. The first three are the truth tables generated by the boolean functions 'AND', 'OR', and 'XOR' on three input variables. We expect our model to learn the 'AND' and 'OR' functions, but not 'XOR'.

The final two datasets are derived from the Iris flower dataset citep:Lichman:2013 . The original dataset has three output classes, one of which ('iris-setosa') is linearly separable from the other two ('iris-versicolor' and 'iris-virginica'), which cannot be linearly separated. On this basis, the dataset is divided into two derived sets with binary class outputs, corresponding to the pairs of linearly and non-linearly variables respectively. The perceptron model is expected to converge only on the linearly separable data.


#+name: and-data
| 0 | 0 | 0 | 0 |
| 0 | 0 | 1 | 0 |
| 0 | 1 | 0 | 0 |
| 0 | 1 | 1 | 0 |
| 1 | 0 | 0 | 0 |
| 1 | 0 | 1 | 0 |
| 1 | 1 | 0 | 0 |
| 1 | 1 | 1 | 1 |


#+name: or-data
| 0 | 0 | 0 | 0 |
| 0 | 0 | 1 | 1 |
| 0 | 1 | 0 | 1 |
| 0 | 1 | 1 | 1 |
| 1 | 0 | 0 | 1 |
| 1 | 0 | 1 | 1 |
| 1 | 1 | 0 | 1 |
| 1 | 1 | 1 | 1 |

#+name: xor-data
| 0 | 0 | 0 | 0 |
| 0 | 0 | 1 | 1 |
| 0 | 1 | 0 | 1 |
| 0 | 1 | 1 | 0 |
| 1 | 0 | 0 | 1 |
| 1 | 0 | 1 | 0 |
| 1 | 1 | 0 | 0 |
| 1 | 1 | 1 | 0 |


#+name: iris-linear
#+BEGIN_SRC python :results value table :exports none
with open("iris.csv") as f:
    data = f.read().split('\n')
    data = [row.split(',') for row in data]
for row in data:
    if row[-1] == "Iris-setosa":
        row[-1] = '1'
    else:
        row[-1] = '0'
    # print("|", "|".join(row), "|")

with open("iris-linear.tsv", 'w') as f:
    print("\n".join(["\t".join(row) for row in data]), file=f, end='')

return data
#+END_SRC

#+RESULTS: iris-linear
| 5.1 | 3.5 | 1.4 | 0.2 | 1 |
| 4.9 | 3.0 | 1.4 | 0.2 | 1 |
| 4.7 | 3.2 | 1.3 | 0.2 | 1 |
| 4.6 | 3.1 | 1.5 | 0.2 | 1 |
| 5.0 | 3.6 | 1.4 | 0.2 | 1 |
| 5.4 | 3.9 | 1.7 | 0.4 | 1 |
| 4.6 | 3.4 | 1.4 | 0.3 | 1 |
| 5.0 | 3.4 | 1.5 | 0.2 | 1 |
| 4.4 | 2.9 | 1.4 | 0.2 | 1 |
| 4.9 | 3.1 | 1.5 | 0.1 | 1 |
| 5.4 | 3.7 | 1.5 | 0.2 | 1 |
| 4.8 | 3.4 | 1.6 | 0.2 | 1 |
| 4.8 | 3.0 | 1.4 | 0.1 | 1 |
| 4.3 | 3.0 | 1.1 | 0.1 | 1 |
| 5.8 | 4.0 | 1.2 | 0.2 | 1 |
| 5.7 | 4.4 | 1.5 | 0.4 | 1 |
| 5.4 | 3.9 | 1.3 | 0.4 | 1 |
| 5.1 | 3.5 | 1.4 | 0.3 | 1 |
| 5.7 | 3.8 | 1.7 | 0.3 | 1 |
| 5.1 | 3.8 | 1.5 | 0.3 | 1 |
| 5.4 | 3.4 | 1.7 | 0.2 | 1 |
| 5.1 | 3.7 | 1.5 | 0.4 | 1 |
| 4.6 | 3.6 | 1.0 | 0.2 | 1 |
| 5.1 | 3.3 | 1.7 | 0.5 | 1 |
| 4.8 | 3.4 | 1.9 | 0.2 | 1 |
| 5.0 | 3.0 | 1.6 | 0.2 | 1 |
| 5.0 | 3.4 | 1.6 | 0.4 | 1 |
| 5.2 | 3.5 | 1.5 | 0.2 | 1 |
| 5.2 | 3.4 | 1.4 | 0.2 | 1 |
| 4.7 | 3.2 | 1.6 | 0.2 | 1 |
| 4.8 | 3.1 | 1.6 | 0.2 | 1 |
| 5.4 | 3.4 | 1.5 | 0.4 | 1 |
| 5.2 | 4.1 | 1.5 | 0.1 | 1 |
| 5.5 | 4.2 | 1.4 | 0.2 | 1 |
| 4.9 | 3.1 | 1.5 | 0.1 | 1 |
| 5.0 | 3.2 | 1.2 | 0.2 | 1 |
| 5.5 | 3.5 | 1.3 | 0.2 | 1 |
| 4.9 | 3.1 | 1.5 | 0.1 | 1 |
| 4.4 | 3.0 | 1.3 | 0.2 | 1 |
| 5.1 | 3.4 | 1.5 | 0.2 | 1 |
| 5.0 | 3.5 | 1.3 | 0.3 | 1 |
| 4.5 | 2.3 | 1.3 | 0.3 | 1 |
| 4.4 | 3.2 | 1.3 | 0.2 | 1 |
| 5.0 | 3.5 | 1.6 | 0.6 | 1 |
| 5.1 | 3.8 | 1.9 | 0.4 | 1 |
| 4.8 | 3.0 | 1.4 | 0.3 | 1 |
| 5.1 | 3.8 | 1.6 | 0.2 | 1 |
| 4.6 | 3.2 | 1.4 | 0.2 | 1 |
| 5.3 | 3.7 | 1.5 | 0.2 | 1 |
| 5.0 | 3.3 | 1.4 | 0.2 | 1 |
| 7.0 | 3.2 | 4.7 | 1.4 | 0 |
| 6.4 | 3.2 | 4.5 | 1.5 | 0 |
| 6.9 | 3.1 | 4.9 | 1.5 | 0 |
| 5.5 | 2.3 | 4.0 | 1.3 | 0 |
| 6.5 | 2.8 | 4.6 | 1.5 | 0 |
| 5.7 | 2.8 | 4.5 | 1.3 | 0 |
| 6.3 | 3.3 | 4.7 | 1.6 | 0 |
| 4.9 | 2.4 | 3.3 | 1.0 | 0 |
| 6.6 | 2.9 | 4.6 | 1.3 | 0 |
| 5.2 | 2.7 | 3.9 | 1.4 | 0 |
| 5.0 | 2.0 | 3.5 | 1.0 | 0 |
| 5.9 | 3.0 | 4.2 | 1.5 | 0 |
| 6.0 | 2.2 | 4.0 | 1.0 | 0 |
| 6.1 | 2.9 | 4.7 | 1.4 | 0 |
| 5.6 | 2.9 | 3.6 | 1.3 | 0 |
| 6.7 | 3.1 | 4.4 | 1.4 | 0 |
| 5.6 | 3.0 | 4.5 | 1.5 | 0 |
| 5.8 | 2.7 | 4.1 | 1.0 | 0 |
| 6.2 | 2.2 | 4.5 | 1.5 | 0 |
| 5.6 | 2.5 | 3.9 | 1.1 | 0 |
| 5.9 | 3.2 | 4.8 | 1.8 | 0 |
| 6.1 | 2.8 | 4.0 | 1.3 | 0 |
| 6.3 | 2.5 | 4.9 | 1.5 | 0 |
| 6.1 | 2.8 | 4.7 | 1.2 | 0 |
| 6.4 | 2.9 | 4.3 | 1.3 | 0 |
| 6.6 | 3.0 | 4.4 | 1.4 | 0 |
| 6.8 | 2.8 | 4.8 | 1.4 | 0 |
| 6.7 | 3.0 | 5.0 | 1.7 | 0 |
| 6.0 | 2.9 | 4.5 | 1.5 | 0 |
| 5.7 | 2.6 | 3.5 | 1.0 | 0 |
| 5.5 | 2.4 | 3.8 | 1.1 | 0 |
| 5.5 | 2.4 | 3.7 | 1.0 | 0 |
| 5.8 | 2.7 | 3.9 | 1.2 | 0 |
| 6.0 | 2.7 | 5.1 | 1.6 | 0 |
| 5.4 | 3.0 | 4.5 | 1.5 | 0 |
| 6.0 | 3.4 | 4.5 | 1.6 | 0 |
| 6.7 | 3.1 | 4.7 | 1.5 | 0 |
| 6.3 | 2.3 | 4.4 | 1.3 | 0 |
| 5.6 | 3.0 | 4.1 | 1.3 | 0 |
| 5.5 | 2.5 | 4.0 | 1.3 | 0 |
| 5.5 | 2.6 | 4.4 | 1.2 | 0 |
| 6.1 | 3.0 | 4.6 | 1.4 | 0 |
| 5.8 | 2.6 | 4.0 | 1.2 | 0 |
| 5.0 | 2.3 | 3.3 | 1.0 | 0 |
| 5.6 | 2.7 | 4.2 | 1.3 | 0 |
| 5.7 | 3.0 | 4.2 | 1.2 | 0 |
| 5.7 | 2.9 | 4.2 | 1.3 | 0 |
| 6.2 | 2.9 | 4.3 | 1.3 | 0 |
| 5.1 | 2.5 | 3.0 | 1.1 | 0 |
| 5.7 | 2.8 | 4.1 | 1.3 | 0 |
| 6.3 | 3.3 | 6.0 | 2.5 | 0 |
| 5.8 | 2.7 | 5.1 | 1.9 | 0 |
| 7.1 | 3.0 | 5.9 | 2.1 | 0 |
| 6.3 | 2.9 | 5.6 | 1.8 | 0 |
| 6.5 | 3.0 | 5.8 | 2.2 | 0 |
| 7.6 | 3.0 | 6.6 | 2.1 | 0 |
| 4.9 | 2.5 | 4.5 | 1.7 | 0 |
| 7.3 | 2.9 | 6.3 | 1.8 | 0 |
| 6.7 | 2.5 | 5.8 | 1.8 | 0 |
| 7.2 | 3.6 | 6.1 | 2.5 | 0 |
| 6.5 | 3.2 | 5.1 | 2.0 | 0 |
| 6.4 | 2.7 | 5.3 | 1.9 | 0 |
| 6.8 | 3.0 | 5.5 | 2.1 | 0 |
| 5.7 | 2.5 | 5.0 | 2.0 | 0 |
| 5.8 | 2.8 | 5.1 | 2.4 | 0 |
| 6.4 | 3.2 | 5.3 | 2.3 | 0 |
| 6.5 | 3.0 | 5.5 | 1.8 | 0 |
| 7.7 | 3.8 | 6.7 | 2.2 | 0 |
| 7.7 | 2.6 | 6.9 | 2.3 | 0 |
| 6.0 | 2.2 | 5.0 | 1.5 | 0 |
| 6.9 | 3.2 | 5.7 | 2.3 | 0 |
| 5.6 | 2.8 | 4.9 | 2.0 | 0 |
| 7.7 | 2.8 | 6.7 | 2.0 | 0 |
| 6.3 | 2.7 | 4.9 | 1.8 | 0 |
| 6.7 | 3.3 | 5.7 | 2.1 | 0 |
| 7.2 | 3.2 | 6.0 | 1.8 | 0 |
| 6.2 | 2.8 | 4.8 | 1.8 | 0 |
| 6.1 | 3.0 | 4.9 | 1.8 | 0 |
| 6.4 | 2.8 | 5.6 | 2.1 | 0 |
| 7.2 | 3.0 | 5.8 | 1.6 | 0 |
| 7.4 | 2.8 | 6.1 | 1.9 | 0 |
| 7.9 | 3.8 | 6.4 | 2.0 | 0 |
| 6.4 | 2.8 | 5.6 | 2.2 | 0 |
| 6.3 | 2.8 | 5.1 | 1.5 | 0 |
| 6.1 | 2.6 | 5.6 | 1.4 | 0 |
| 7.7 | 3.0 | 6.1 | 2.3 | 0 |
| 6.3 | 3.4 | 5.6 | 2.4 | 0 |
| 6.4 | 3.1 | 5.5 | 1.8 | 0 |
| 6.0 | 3.0 | 4.8 | 1.8 | 0 |
| 6.9 | 3.1 | 5.4 | 2.1 | 0 |
| 6.7 | 3.1 | 5.6 | 2.4 | 0 |
| 6.9 | 3.1 | 5.1 | 2.3 | 0 |
| 5.8 | 2.7 | 5.1 | 1.9 | 0 |
| 6.8 | 3.2 | 5.9 | 2.3 | 0 |
| 6.7 | 3.3 | 5.7 | 2.5 | 0 |
| 6.7 | 3.0 | 5.2 | 2.3 | 0 |
| 6.3 | 2.5 | 5.0 | 1.9 | 0 |
| 6.5 | 3.0 | 5.2 | 2.0 | 0 |
| 6.2 | 3.4 | 5.4 | 2.3 | 0 |
| 5.9 | 3.0 | 5.1 | 1.8 | 0 |

#+name: iris-nonlinear
#+BEGIN_SRC python :results value table :exports none
with open("iris.csv") as f:
    data = f.read().split('\n')
    data = [row.split(',') for row in data]
for row in data:
    if row[-1] == "Iris-virginica":
        row[-1] = '1'
    else:
        row[-1] = '0'
#     print("|", "|".join(row), "|")
return data
#+END_SRC

#+RESULTS: iris-nonlinear
| 5.1 | 3.5 | 1.4 | 0.2 | 0 |
| 4.9 | 3.0 | 1.4 | 0.2 | 0 |
| 4.7 | 3.2 | 1.3 | 0.2 | 0 |
| 4.6 | 3.1 | 1.5 | 0.2 | 0 |
| 5.0 | 3.6 | 1.4 | 0.2 | 0 |
| 5.4 | 3.9 | 1.7 | 0.4 | 0 |
| 4.6 | 3.4 | 1.4 | 0.3 | 0 |
| 5.0 | 3.4 | 1.5 | 0.2 | 0 |
| 4.4 | 2.9 | 1.4 | 0.2 | 0 |
| 4.9 | 3.1 | 1.5 | 0.1 | 0 |
| 5.4 | 3.7 | 1.5 | 0.2 | 0 |
| 4.8 | 3.4 | 1.6 | 0.2 | 0 |
| 4.8 | 3.0 | 1.4 | 0.1 | 0 |
| 4.3 | 3.0 | 1.1 | 0.1 | 0 |
| 5.8 | 4.0 | 1.2 | 0.2 | 0 |
| 5.7 | 4.4 | 1.5 | 0.4 | 0 |
| 5.4 | 3.9 | 1.3 | 0.4 | 0 |
| 5.1 | 3.5 | 1.4 | 0.3 | 0 |
| 5.7 | 3.8 | 1.7 | 0.3 | 0 |
| 5.1 | 3.8 | 1.5 | 0.3 | 0 |
| 5.4 | 3.4 | 1.7 | 0.2 | 0 |
| 5.1 | 3.7 | 1.5 | 0.4 | 0 |
| 4.6 | 3.6 | 1.0 | 0.2 | 0 |
| 5.1 | 3.3 | 1.7 | 0.5 | 0 |
| 4.8 | 3.4 | 1.9 | 0.2 | 0 |
| 5.0 | 3.0 | 1.6 | 0.2 | 0 |
| 5.0 | 3.4 | 1.6 | 0.4 | 0 |
| 5.2 | 3.5 | 1.5 | 0.2 | 0 |
| 5.2 | 3.4 | 1.4 | 0.2 | 0 |
| 4.7 | 3.2 | 1.6 | 0.2 | 0 |
| 4.8 | 3.1 | 1.6 | 0.2 | 0 |
| 5.4 | 3.4 | 1.5 | 0.4 | 0 |
| 5.2 | 4.1 | 1.5 | 0.1 | 0 |
| 5.5 | 4.2 | 1.4 | 0.2 | 0 |
| 4.9 | 3.1 | 1.5 | 0.1 | 0 |
| 5.0 | 3.2 | 1.2 | 0.2 | 0 |
| 5.5 | 3.5 | 1.3 | 0.2 | 0 |
| 4.9 | 3.1 | 1.5 | 0.1 | 0 |
| 4.4 | 3.0 | 1.3 | 0.2 | 0 |
| 5.1 | 3.4 | 1.5 | 0.2 | 0 |
| 5.0 | 3.5 | 1.3 | 0.3 | 0 |
| 4.5 | 2.3 | 1.3 | 0.3 | 0 |
| 4.4 | 3.2 | 1.3 | 0.2 | 0 |
| 5.0 | 3.5 | 1.6 | 0.6 | 0 |
| 5.1 | 3.8 | 1.9 | 0.4 | 0 |
| 4.8 | 3.0 | 1.4 | 0.3 | 0 |
| 5.1 | 3.8 | 1.6 | 0.2 | 0 |
| 4.6 | 3.2 | 1.4 | 0.2 | 0 |
| 5.3 | 3.7 | 1.5 | 0.2 | 0 |
| 5.0 | 3.3 | 1.4 | 0.2 | 0 |
| 7.0 | 3.2 | 4.7 | 1.4 | 0 |
| 6.4 | 3.2 | 4.5 | 1.5 | 0 |
| 6.9 | 3.1 | 4.9 | 1.5 | 0 |
| 5.5 | 2.3 | 4.0 | 1.3 | 0 |
| 6.5 | 2.8 | 4.6 | 1.5 | 0 |
| 5.7 | 2.8 | 4.5 | 1.3 | 0 |
| 6.3 | 3.3 | 4.7 | 1.6 | 0 |
| 4.9 | 2.4 | 3.3 | 1.0 | 0 |
| 6.6 | 2.9 | 4.6 | 1.3 | 0 |
| 5.2 | 2.7 | 3.9 | 1.4 | 0 |
| 5.0 | 2.0 | 3.5 | 1.0 | 0 |
| 5.9 | 3.0 | 4.2 | 1.5 | 0 |
| 6.0 | 2.2 | 4.0 | 1.0 | 0 |
| 6.1 | 2.9 | 4.7 | 1.4 | 0 |
| 5.6 | 2.9 | 3.6 | 1.3 | 0 |
| 6.7 | 3.1 | 4.4 | 1.4 | 0 |
| 5.6 | 3.0 | 4.5 | 1.5 | 0 |
| 5.8 | 2.7 | 4.1 | 1.0 | 0 |
| 6.2 | 2.2 | 4.5 | 1.5 | 0 |
| 5.6 | 2.5 | 3.9 | 1.1 | 0 |
| 5.9 | 3.2 | 4.8 | 1.8 | 0 |
| 6.1 | 2.8 | 4.0 | 1.3 | 0 |
| 6.3 | 2.5 | 4.9 | 1.5 | 0 |
| 6.1 | 2.8 | 4.7 | 1.2 | 0 |
| 6.4 | 2.9 | 4.3 | 1.3 | 0 |
| 6.6 | 3.0 | 4.4 | 1.4 | 0 |
| 6.8 | 2.8 | 4.8 | 1.4 | 0 |
| 6.7 | 3.0 | 5.0 | 1.7 | 0 |
| 6.0 | 2.9 | 4.5 | 1.5 | 0 |
| 5.7 | 2.6 | 3.5 | 1.0 | 0 |
| 5.5 | 2.4 | 3.8 | 1.1 | 0 |
| 5.5 | 2.4 | 3.7 | 1.0 | 0 |
| 5.8 | 2.7 | 3.9 | 1.2 | 0 |
| 6.0 | 2.7 | 5.1 | 1.6 | 0 |
| 5.4 | 3.0 | 4.5 | 1.5 | 0 |
| 6.0 | 3.4 | 4.5 | 1.6 | 0 |
| 6.7 | 3.1 | 4.7 | 1.5 | 0 |
| 6.3 | 2.3 | 4.4 | 1.3 | 0 |
| 5.6 | 3.0 | 4.1 | 1.3 | 0 |
| 5.5 | 2.5 | 4.0 | 1.3 | 0 |
| 5.5 | 2.6 | 4.4 | 1.2 | 0 |
| 6.1 | 3.0 | 4.6 | 1.4 | 0 |
| 5.8 | 2.6 | 4.0 | 1.2 | 0 |
| 5.0 | 2.3 | 3.3 | 1.0 | 0 |
| 5.6 | 2.7 | 4.2 | 1.3 | 0 |
| 5.7 | 3.0 | 4.2 | 1.2 | 0 |
| 5.7 | 2.9 | 4.2 | 1.3 | 0 |
| 6.2 | 2.9 | 4.3 | 1.3 | 0 |
| 5.1 | 2.5 | 3.0 | 1.1 | 0 |
| 5.7 | 2.8 | 4.1 | 1.3 | 0 |
| 6.3 | 3.3 | 6.0 | 2.5 | 1 |
| 5.8 | 2.7 | 5.1 | 1.9 | 1 |
| 7.1 | 3.0 | 5.9 | 2.1 | 1 |
| 6.3 | 2.9 | 5.6 | 1.8 | 1 |
| 6.5 | 3.0 | 5.8 | 2.2 | 1 |
| 7.6 | 3.0 | 6.6 | 2.1 | 1 |
| 4.9 | 2.5 | 4.5 | 1.7 | 1 |
| 7.3 | 2.9 | 6.3 | 1.8 | 1 |
| 6.7 | 2.5 | 5.8 | 1.8 | 1 |
| 7.2 | 3.6 | 6.1 | 2.5 | 1 |
| 6.5 | 3.2 | 5.1 | 2.0 | 1 |
| 6.4 | 2.7 | 5.3 | 1.9 | 1 |
| 6.8 | 3.0 | 5.5 | 2.1 | 1 |
| 5.7 | 2.5 | 5.0 | 2.0 | 1 |
| 5.8 | 2.8 | 5.1 | 2.4 | 1 |
| 6.4 | 3.2 | 5.3 | 2.3 | 1 |
| 6.5 | 3.0 | 5.5 | 1.8 | 1 |
| 7.7 | 3.8 | 6.7 | 2.2 | 1 |
| 7.7 | 2.6 | 6.9 | 2.3 | 1 |
| 6.0 | 2.2 | 5.0 | 1.5 | 1 |
| 6.9 | 3.2 | 5.7 | 2.3 | 1 |
| 5.6 | 2.8 | 4.9 | 2.0 | 1 |
| 7.7 | 2.8 | 6.7 | 2.0 | 1 |
| 6.3 | 2.7 | 4.9 | 1.8 | 1 |
| 6.7 | 3.3 | 5.7 | 2.1 | 1 |
| 7.2 | 3.2 | 6.0 | 1.8 | 1 |
| 6.2 | 2.8 | 4.8 | 1.8 | 1 |
| 6.1 | 3.0 | 4.9 | 1.8 | 1 |
| 6.4 | 2.8 | 5.6 | 2.1 | 1 |
| 7.2 | 3.0 | 5.8 | 1.6 | 1 |
| 7.4 | 2.8 | 6.1 | 1.9 | 1 |
| 7.9 | 3.8 | 6.4 | 2.0 | 1 |
| 6.4 | 2.8 | 5.6 | 2.2 | 1 |
| 6.3 | 2.8 | 5.1 | 1.5 | 1 |
| 6.1 | 2.6 | 5.6 | 1.4 | 1 |
| 7.7 | 3.0 | 6.1 | 2.3 | 1 |
| 6.3 | 3.4 | 5.6 | 2.4 | 1 |
| 6.4 | 3.1 | 5.5 | 1.8 | 1 |
| 6.0 | 3.0 | 4.8 | 1.8 | 1 |
| 6.9 | 3.1 | 5.4 | 2.1 | 1 |
| 6.7 | 3.1 | 5.6 | 2.4 | 1 |
| 6.9 | 3.1 | 5.1 | 2.3 | 1 |
| 5.8 | 2.7 | 5.1 | 1.9 | 1 |
| 6.8 | 3.2 | 5.9 | 2.3 | 1 |
| 6.7 | 3.3 | 5.7 | 2.5 | 1 |
| 6.7 | 3.0 | 5.2 | 2.3 | 1 |
| 6.3 | 2.5 | 5.0 | 1.9 | 1 |
| 6.5 | 3.0 | 5.2 | 2.0 | 1 |
| 6.2 | 3.4 | 5.4 | 2.3 | 1 |
| 5.9 | 3.0 | 5.1 | 1.8 | 1 |


* Model
The model implementation follows the description of the perceptron learning algorithm in Ch.4 of [[citet:rojas-1996]]. The model is initialised with randomised weights corresponding to the number of input variables. A bias value is added to the weight vector, and the input vector extended by 1 to allow learning of this parameter.

The activation function of the model is defined as the dot product of the input and weight vectors. The model predicts a positive class if the activation function returns a positive value, and a negative class otherwise. During training, the error for each row of inputs is calculated as the difference of the true output and the predicted output. For each row of input, \(x\), the weight vector \(w\) is adjusted by an amount \(learning-constant * error * x\). Training is continued until either the model converges or the specified number of iterations are completed. The weights corresponding to the lowest error on the training data are stored.

Python code for the implementation is provided in Cref:perceptron-class.

* Results

Now we present graphs of the model's error rate while training on the different datasets.

#+name: plot-and
#+BEGIN_SRC ipython :results raw :ob-ipython-results image/png :exports results :var data = and-data
import numpy as np
import sys
from perceptron import Perceptron
from matplotlib import pyplot as plt


%matplotlib inline

model = Perceptron(data)
model.fit(lr = 0.5, numIters = 1000)
plt.plot(model.fitHistory)
plt.xlabel("Iterations")
plt.ylabel("Error")
print('#+NAME: fig-1')
print('#+CAPTION: Learning the AND function', end='')
#+END_SRC

#+RESULTS: plot-and
#+NAME: fig-1
#+CAPTION: Learning the AND function
[[file:ipython-inline-images/ob-ipython-8d732c9879226ae9c7e4c80e161e8764.png]]


#+name: plot-or
#+BEGIN_SRC ipython :results raw :ob-ipython-results image/png :exports results :var data = or-data
import numpy as np
import sys
from perceptron import Perceptron
from matplotlib import pyplot as plt


%matplotlib inline

model = Perceptron(data)
model.fit(lr = .5, numIters = 1000)
plt.plot(model.fitHistory)
plt.xlabel("Iterations")
plt.ylabel("Error")
print('#+NAME: fig-2')
print('#+CAPTION: Learning the OR function', end='')
#+END_SRC

#+RESULTS: plot-or
#+NAME: fig-2
#+CAPTION: Learning the OR function
[[file:ipython-inline-images/ob-ipython-6b49594bc3f281d46f9563e9d2bd8c58.png]]


#+name: plot-xor
#+BEGIN_SRC ipython :results raw :ob-ipython-results image/png :exports results :var data = xor-data
import numpy as np
import sys
from perceptron import Perceptron
from matplotlib import pyplot as plt


%matplotlib inline

model = Perceptron(data)
model.fit(lr = 0.5, numIters = 1000)
plt.plot(model.fitHistory)
plt.xlabel("Iterations")
plt.ylabel("Error")
print('#+NAME: fig-3')
print('#+CAPTION: Learning the XOR function', end='')
#+END_SRC

#+RESULTS: plot-xor
#+NAME: fig-3
#+CAPTION: Learning the XOR function
[[file:ipython-inline-images/ob-ipython-399086004ebd7aaba2f750d7d4a0be86.png]]


#+name: plot-iris-linear
#+BEGIN_SRC ipython :results raw :ob-ipython-results image/png :exports results :var data = iris-linear
import numpy as np
import sys
from perceptron import Perceptron
from matplotlib import pyplot as plt


%matplotlib inline

model = Perceptron(data)
model.fit(lr = 0.01, numIters = 1000)
plt.plot(model.fitHistory)
plt.xlabel("Iterations")
plt.ylabel("Error")
print('#+NAME: fig-4')
print("#+CAPTION: Training on the Iris 'linear' dataset", end='')
#+END_SRC

#+RESULTS: plot-iris-linear
#+NAME: fig-4
#+CAPTION: Training on the Iris 'linear' dataset
[[file:ipython-inline-images/ob-ipython-4651108eb17349d8601089953e8ef051.png]]


#+name: plot-iris-non-linear
#+BEGIN_SRC ipython :results raw :ob-ipython-results image/png :exports results :var data = iris-nonlinear
import numpy as np
import sys
from perceptron import Perceptron
from matplotlib import pyplot as plt


%matplotlib inline

model = Perceptron(data)
model.fit(lr = 0.01, numIters = 1000)
plt.plot(model.fitHistory)
plt.xlabel("Iterations")
plt.ylabel("Error")
print('#+NAME: fig-5')
print("#+CAPTION: Training on the Iris 'non-linear' data", end='')
#+END_SRC

#+RESULTS: plot-iris-non-linear
#+NAME: fig-5
#+CAPTION: Training on the Iris 'non-linear' data
[[file:ipython-inline-images/ob-ipython-b07496e3f8b25a52a30e255ac7cff196.png]]


We can conclude from these plots that our perceptron model is able to quickly converge on the three linearly separable datasets, namely the AND/OR function tables, and the linearly separable classes of the Iris data (Cref:fig-1,fig-2,fig-4). However, the model fails to train successfully on the XOR function data or the non-linearly separable classes of the Iris data (Cref:fig-3,fig-5) even after training for 1000 iterations. Clearly, a simple perceptron model is only able to function as a linear classifier.

bibliography:perceptron.bib
bibliographystyle:acl

\newpage
\appendix

#+caption: Model Implementation label:perceptron-class
#+BEGIN_SRC ipython :results silent :exports code :tangle perceptron.py
import random
import numpy as np
import sys


class Perceptron:
    def __init__(self, data, weights = None):
        random.shuffle(data)
        inputs = np.array([[float(x) for x in row[0:-1]] for row in data])
        self.inputs = np.hstack((inputs, [[1]] * len(inputs))) # Append 1 to each input row, for the bias weight
        self.outputs = np.array([float(row[-1]) for row in data])
        self.numInputs = len(self.inputs[0])
        if weights == None:
            weights = np.array([random.uniform(0, 100) \
                                     for x in range(self.numInputs)])
            weights[-1] = -1 # set initial value of bias weight
        self.weights = weights
        self.error = float(sys.maxsize) # initialise error to some very high value
        self.smallestError = self.error
        self.bestWeights = self.weights
        self.fitHistory = []

    def predict(self, x_i):
        y = np.dot(x_i, self.weights) # Activation function is the dot product of input vector and weight vector
        return 1 if y > 0 else 0

    def fit(self, lr=1, numIters = 100, breakSoon=True):
        errorList = []
        for iter in range(numIters):
            totalError = 0.0
            for i in range(len(self.outputs)):
                pred = self.predict(self.inputs[i])
                error = self.outputs[i] - pred # Error is the difference between true and predicted class
                self.weights = self.weights + \
                               lr * error * self.inputs[i] # multiplying with the error yields a positive or negative adjustment depending on a positive or negative prediction error
                totalError += abs(error)
            
            self.saveBestFit(self.weights, totalError)
            if breakSoon:
                if totalError == 0.0:
                    break
            self.printWeights()
            errorList.append(totalError)

        self.fitHistory = errorList # Store error history for convenient plotting
        self.error = totalError
        
    def saveBestFit(self, w, e): # Store the best performing weights for reuse
        if e < self.smallestError:
            self.smallestError = e
            self.bestWeights = w

    def printWeights(self):
        print("\t".join(map(str, self.weights)), file=sys.stderr)

    def test(self): # Ideally we should split data into train/test sets to feed this method. For now, just use the data passed during initialisation.
        e = 0.0
        for i in range(len(self.inputs)):
            pred = self.predict(self.inputs[i])
            e += self.outputs[i] - pred
        print(e, file=sys.stdout)
        
    def __str__(self):
        s = "inputs (1 sample): {}\n".format(self.inputs[0])
        s += "weights: {}\n".format(self.weights)
        s += "error: {}\n".format(self.error)
        return s
#+END_SRC

#+name: train-script
#+BEGIN_SRC python :results silent :exports none :tangle train.py
import sys
from perceptron import Perceptron

if __name__ == "__main__":
    iters = int(sys.argv[1])
    lr = float(sys.argv[2])
    with open(sys.argv[3]) as f:
        data = f.read().split('\n')
        data = [row.split('\t') for row in data]
    model = Perceptron(data)

    model.fit(lr, iters, True)
    print("\t".join(map(str, model.bestWeights)), file=sys.stdout)
#+END_SRC

#+name: test-script
#+BEGIN_SRC python :results silent :exports none :tangle test.py
import sys
from perceptron import Perceptron

if __name__ == "__main__":
    with open(sys.argv[1]) as f:
        data = f.read().split('\n')
        data = [row.split('\t') for row in data]
    weights = sys.argv[2:]
    weights = [float(weight) for weight in weights]
    model = Perceptron(data, weights)
    model.test()
#+END_SRC
