% Created 2018-02-11 Sun 02:49
% Intended LaTeX compiler: pdflatex
\documentclass[11pt,a4paper]{article}
  \usepackage[utf8]{inputenc}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{minted}
\usepackage{float}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{url}
\usepackage{cleveref}
\aclfinalcopy
\author{Irfan S \\ Dept. of Linguitics / UIUC \\ {\tt irfans2@illinois.edu}}
\newcommand\BibTeX{B{\sc ib}\TeX}
\date{\today}
\title{Single Cell Perceptron in Python: Implementation and Results}
\begin{document}

\maketitle

\section{Data}
\label{sec:orge7f12f1}
We will be using five datasets to train and observe the model. The first three are the truth tables generated by the boolean functions 'AND', 'OR', and 'XOR' on three input variables. We expect our model to learn the 'AND' and 'OR' functions, but not 'XOR'.

The final two datasets are derived from the Iris flower dataset \citep{Lichman:2013} . The original dataset has three output classes, one of which ('iris-setosa') is linearly separable from the other two ('iris-versicolor' and 'iris-virginica'), which cannot be linearly separated. On this basis, the dataset is divided into two derived sets with binary class outputs, corresponding to the pairs of linearly and non-linearly variables respectively. The perceptron model is expected to converge only on the linearly separable data.


\section{Model}
\label{sec:org50561b5}
The model implementation follows the description of the perceptron learning algorithm in Ch.4 of \citet{rojas-1996}. The model is initialised with randomised weights corresponding to the number of input variables. A bias value is added to the weight vector, and the input vector extended by 1 to allow learning of this parameter.

The activation function of the model is defined as the dot product of the input and weight vectors. The model predicts a positive class if the activation function returns a positive value, and a negative class otherwise. During training, the error for each row of inputs is calculated as the difference of the true output and the predicted output. For each row of input, \(x\), the weight vector \(w\) is adjusted by an amount \(learning-constant * error * x\). Training is continued until either the model converges or the specified number of iterations are completed. The weights corresponding to the lowest error on the training data are stored.

Python code for the implementation is provided in \Cref{perceptron-class}.

\section{Results}
\label{sec:org6c0f29d}

Now we present graphs of the model's error rate while training on the different datasets.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{ipython-inline-images/ob-ipython-25e4eacca27ef459c86105b99cb9ffe9.png}
\caption{\label{fig-1}
Learning the AND function}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{ipython-inline-images/ob-ipython-11a383b697a09fe87a69158ab732c17d.png}
\caption{\label{fig-2}
Learning the OR function}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{ipython-inline-images/ob-ipython-1298f744cb7b690d1dc278acd881bc1b.png}
\caption{\label{fig-3}
Learning the XOR function}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{ipython-inline-images/ob-ipython-ba894552f333d60211347aa89aee74ab.png}
\caption{\label{fig-4}
Training on the Iris 'linear' dataset}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{ipython-inline-images/ob-ipython-7243bc05735719ddc5df13c0e560cfb4.png}
\caption{\label{fig-5}
Training on the Iris 'non-linear' data}
\end{figure}


We can see through these plots that our perceptron model is able to quickly converge on the three linearly separable datasets, namely the AND/OR function tables, and the linearly separable classes of the Iris data (\Cref{fig-1,fig-2,fig-4}). However, the model fails to train successfully on the XOR function data or the non-linearly separable classes of the Iris data (\Cref{fig-3,fig-5}) even after training for 1000 iterations. Clearly, a simple perceptron model is only able to function as a linear classifier.

\bibliography{perceptron}
\bibliographystyle{acl}

\newpage
\appendix

\begin{listing}[htbp]
\begin{minted}[frame=lines,fontsize=\scriptsize,linenos]{ipython}
import random
import numpy as np
import sys


class Perceptron:
    def __init__(self, data, weights = None):
        random.shuffle(data)
        inputs = np.array([[float(x) for x in row[0:-1]] for row in data])
        self.inputs = np.hstack((inputs, [[1]] * len(inputs))) # Append 1 to each input row, for the bias weight
        self.outputs = np.array([float(row[-1]) for row in data])
        self.numInputs = len(self.inputs[0])
        if weights == None:
            weights = np.array([random.uniform(0, 100) \
                                     for x in range(self.numInputs)])
            weights[-1] = -1 # set initial value of bias weight
        self.weights = weights
        self.error = float(sys.maxsize) # initialise error to some very high value
        self.smallestError = self.error
        self.bestWeights = self.weights
        self.fitHistory = []

    def predict(self, x_i):
        y = np.dot(x_i, self.weights) # Activation function is the dot product of input vector and weight vector
        return 1 if y > 0 else 0

    def fit(self, lr=1, numIters = 100, breakSoon=True):
        errorList = []
        for iter in range(numIters):
            totalError = 0.0
            for i in range(len(self.outputs)):
                pred = self.predict(self.inputs[i])
                error = self.outputs[i] - pred # Error is the difference between true and predicted class
                self.weights = self.weights + \
                               lr * error * self.inputs[i] # multiplying with the error yields a positive or negative adjustment depending on a positive or negative prediction error
                totalError += abs(error)
            
            self.saveBestFit(self.weights, totalError)
            if breakSoon:
                if totalError == 0.0:
                    break
            self.printWeights()
            errorList.append(totalError)

        self.fitHistory = errorList # Store error history for convenient plotting
        self.error = totalError
        
    def saveBestFit(self, w, e): # Store the best performing weights for reuse
        if e < self.smallestError:
            self.smallestError = e
            self.bestWeights = w

    def printWeights(self):
        print("\t".join(map(str, self.weights)), file=sys.stderr)

    def test(self): # Ideally we should split data into train/test sets to feed this method. For now, just use the data passed during initialisation.
        e = 0.0
        for i in range(len(self.inputs)):
            pred = self.predict(self.inputs[i])
            e += self.outputs[i] - pred
        print(e, file=sys.stdout)
        
    def __str__(self):
        s = "inputs (1 sample): {}\n".format(self.inputs[0])
        s += "weights: {}\n".format(self.weights)
        s += "error: {}\n".format(self.error)
        return s
\end{minted}
\caption{Model Implementation \label{perceptron-class}}
\end{listing}
\end{document}